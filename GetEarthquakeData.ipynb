{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Andreas Fault seismic analysis\n",
    "\n",
    "This is a script to download earthquake data from USGS ComCat.\n",
    "Firstly it downloads the earthquake events within the specific locations bounds and with the magnitude filters.\n",
    "Then it gets the focal mechanism for each one to determine event type.\n",
    "\n",
    "Uses the libcomcat python wrappers (https://github.com/usgs/libcomcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib imports\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy.geodetics.base import gps2dist_azimuth\n",
    "import pandas as pd\n",
    "\n",
    "# Local imports\n",
    "from libcomcat.dataframes import get_history_data_frame\n",
    "from libcomcat.search import (get_event_by_id, get_authoritative_info, search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to download the general earthquake data from the ComCat database. We set the latitude and longitude bounds of the study area and the minimum and maximum magnitude for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude and longitude bounds\n",
    "bounds = [-125.2, -115.312, 32.946, 40.84]\n",
    "\n",
    "# start/end times for analysis\n",
    "stime = datetime(2010,1,1)\n",
    "etime = datetime.utcnow()\n",
    "\n",
    "# magnitude range\n",
    "minmag = 4.5\n",
    "maxmag = 9.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we download a list of SummaryEvent objects for each earthquake. We use the get_authoritative_info() function to retrieve the authoritative magnitudes, locations and depths from each contributing network. Then that information is assembled into a pandas DataFrame to be used for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "Elapsed time to retrieve 110 magnitudes: 73.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# Retrieve list of events\n",
    "eventlist = search(starttime=stime,\n",
    "                  endtime=etime,\n",
    "                  minlatitude=bounds[2],\n",
    "                  maxlatitude=bounds[3],\n",
    "                  minlongitude=bounds[0],\n",
    "                  maxlongitude=bounds[1],\n",
    "                  minmagnitude=minmag,\n",
    "                  maxmagnitude=maxmag,\n",
    "                  eventtype='earthquake')\n",
    "print(len(eventlist))\n",
    "\n",
    "# Retrieve all mag/locations for those events (depending on bandwidth), this can take 30 seconds to a minute per event.\n",
    "t1 = time()\n",
    "magdict = {}\n",
    "columns = []\n",
    "for event in eventlist:\n",
    "    mdict, ldict, _ = get_authoritative_info(event.id)\n",
    "    mdict.update(ldict)\n",
    "    magdict[event.id] = mdict\n",
    "    columns += list(mdict.keys())\n",
    "    \n",
    "t2 = time()\n",
    "dt = t2-t1\n",
    "fmt = 'Elapsed time to retrieve %i magnitudes: %.2f seconds'\n",
    "print(fmt % (len(eventlist),dt))\n",
    "\n",
    "# Assemble these dictionaries into a DataFrame\n",
    "columns = ['eventid'] + list(set(columns))\n",
    "df = pd.DataFrame(columns=columns)\n",
    "for eventid, edict in magdict.items():\n",
    "    edict['eventid'] = eventid\n",
    "    df = df.append(edict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventid         110\n",
       "nc-longitude     34\n",
       "nc-mw            31\n",
       "ci-ml             2\n",
       "nc-ml             3\n",
       "ci-mw            37\n",
       "us-depth         77\n",
       "us-NA             0\n",
       "ci-depth         62\n",
       "us-ml             1\n",
       "us-mwc            2\n",
       "ci-mh             9\n",
       "ci-longitude     62\n",
       "us-mb             8\n",
       "nc-depth         34\n",
       "us-latitude      77\n",
       "nn-ml            34\n",
       "nn-latitude      34\n",
       "ci-latitude      62\n",
       "us-longitude     77\n",
       "us-mwb            1\n",
       "us-mwr           13\n",
       "nc-latitude      34\n",
       "nn-depth         34\n",
       "us-mww           18\n",
       "nn-longitude     34\n",
       "ci-mlr           14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many non-null values are in each column\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
